{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srucfWs3Aug8"
   },
   "source": [
    "# Building an AI Text-to-Video Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Rd6I9pGAug-"
   },
   "source": [
    "## What is being built?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i3PuYyoAug-"
   },
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6h3oJzGEH0xrER2Tv8M7KQ.gif' width='600'>\n",
    "\n",
    "Such training datasets require extremely high computational power. Therefore, we will work with a video dataset of moving objects generated from Python code.\n",
    "\n",
    "Use the GAN (Generative Adversarial Networks) architecture to create final model due to it's easier and quicker to train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQmcSOelAug_"
   },
   "source": [
    "## GAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEwl8AOOAug_"
   },
   "source": [
    "### What is GAN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4WeA7BrAuhA"
   },
   "source": [
    "Generative Adversarial Network (GAN) is a deep learning model where 2 neural networks compete:\n",
    "- **generator**: creates new data (like images or music) from a given dataset\n",
    "- **discriminator** tell if the data is real or fake.\n",
    "\n",
    "This process continues until the generated data is indistinguishable from the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCfRm3sKAuhA"
   },
   "source": [
    "### Real-World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXs7RbQLAuhA"
   },
   "source": [
    "1. **Generate Images**: GANs create realistic images from text prompts or modify existing images, such as enhancing resolution or adding color to black-and-white photos.\n",
    "2. **Data Augmentation**: They generate synthetic data to train other machine learning models, such as creating fraudulent transaction data for fraud detection systems.\n",
    "3. **Complete Missing Information**: GANs can fill in missing data, like generating sub-surface images from terrain maps for energy applications.\n",
    "4. **Generate 3D Models**: They convert 2D images into 3D models, useful in fields like healthcare for creating realistic organ images for surgical planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srWQyEJqAuhA"
   },
   "source": [
    "### How does a GAN work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEYWfEt_AuhB"
   },
   "source": [
    "1. **Training Set Analysis**: The generator analyzes the training set to identify data attributes, while the discriminator independently analyzes the same data to learn its attributes.\n",
    "2. **Data Modification**: The generator adds noise (random changes) to some attributes of the data.\n",
    "3. **Data Passing**: The modified data is then passed to the discriminator.\n",
    "4. **Probability Calculation**: The discriminator calculates the probability that the generated data is from the original dataset.\n",
    "5. **Feedback Loop**: The discriminator provides feedback to the generator, guiding it to reduce random noise in the next cycle.\n",
    "6. **Adversarial Training**: The generator tries to maximize the discriminator’s mistakes, while the discriminator tries to minimize its own errors. Through many training iterations, both networks improve and evolve.\n",
    "7. **Equilibrium State**: Training continues until the discriminator can no longer distinguish between real and synthesized data, indicating that the generator has successfully learned to produce realistic data. At this point, the training process is complete.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:2000/format:webp/1*2HsK-UFPRvCdAmQyS3Ol1Q.jpeg' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwVSZJivAuhB"
   },
   "source": [
    "### GAN training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL0bqyjuAuhC"
   },
   "source": [
    "An example of image-to-image translation, focusing on modifying a human face.\n",
    "\n",
    "1. Input Image: The input is a real image of a human face.\n",
    "2. Attribute Modification: The generator modifies attributes of the face, like adding sunglasses to the eyes.\n",
    "3. Generated Images: The generator creates a set of images with sunglasses added.\n",
    "4. Discriminator’s Task: The discriminator receives a mix of real images (people with sunglasses) and generated images (faces where sunglasses were added).\n",
    "5. Evaluation: The discriminator tries to differentiate between real and generated images.\n",
    "6. Feedback Loop:\n",
    "- If the discriminator correctly identifies fake images, the generator adjusts its parameters to produce more convincing images.\n",
    "- If the generator successfully fools the discriminator, the discriminator updates its parameters to improve its detection.\n",
    "\n",
    "Through this adversarial process, both networks continuously improve:\n",
    "- The generator gets better at creating realistic images.\n",
    "- The discriminator gets better at identifying fakes.\n",
    "\n",
    "Until equilibrium is reached, where the discriminator can no longer tell the difference between real and generated images. At this point, the GAN has successfully learned to produce realistic modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO9D36WuAuhC"
   },
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6VgzXMqwAuhC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1pl9_dXAuhD"
   },
   "source": [
    "## Creating the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT73409GAuhD"
   },
   "source": [
    "Our training video dataset consists of 1-sec-long 10,000 videos of a circle moving in different directions with different motions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cSsmXcVOAuhD"
   },
   "outputs": [],
   "source": [
    "os.makedirs('training_dataset', exist_ok=True)\n",
    "num_videos = 10000\n",
    "frames_per_video = 10\n",
    "img_size = (64, 64) # the size of each image in the dataset\n",
    "shape_size = 10 # the size of the Circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7Mg4_pgAuhD"
   },
   "source": [
    "Define the text prompts of the training dataset based on which training videos will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LNFS74dHAuhE"
   },
   "outputs": [],
   "source": [
    "# Define text prompts and corresponding movements for circles\n",
    "prompts_and_movements = [\n",
    "    (\"circle moving down\", \"circle\", \"down\"),  # Move circle downward\n",
    "    (\"circle moving left\", \"circle\", \"left\"),  # Move circle leftward\n",
    "    (\"circle moving right\", \"circle\", \"right\"),  # Move circle rightward\n",
    "    (\"circle moving diagonally up-right\", \"circle\", \"diagonal_up_right\"),  # Move circle diagonally up-right\n",
    "    (\"circle moving diagonally down-left\", \"circle\", \"diagonal_down_left\"),  # Move circle diagonally down-left\n",
    "    (\"circle moving diagonally up-left\", \"circle\", \"diagonal_up_left\"),  # Move circle diagonally up-left\n",
    "    (\"circle moving diagonally down-right\", \"circle\", \"diagonal_down_right\"),  # Move circle diagonally down-right\n",
    "    (\"circle rotating clockwise\", \"circle\", \"rotate_clockwise\"),  # Rotate circle clockwise\n",
    "    (\"circle rotating counter-clockwise\", \"circle\", \"rotate_counter_clockwise\"),  # Rotate circle counter-clockwise\n",
    "    (\"circle shrinking\", \"circle\", \"shrink\"),  # Shrink circle\n",
    "    (\"circle expanding\", \"circle\", \"expand\"),  # Expand circle\n",
    "    (\"circle bouncing vertically\", \"circle\", \"bounce_vertical\"),  # Bounce circle vertically\n",
    "    (\"circle bouncing horizontally\", \"circle\", \"bounce_horizontal\"),  # Bounce circle horizontally\n",
    "    (\"circle zigzagging vertically\", \"circle\", \"zigzag_vertical\"),  # Zigzag circle vertically\n",
    "    (\"circle zigzagging horizontally\", \"circle\", \"zigzag_horizontal\"),  # Zigzag circle horizontally\n",
    "    (\"circle moving up-left\", \"circle\", \"up_left\"),  # Move circle up-left\n",
    "    (\"circle moving down-right\", \"circle\", \"down_right\"),  # Move circle down-right\n",
    "    (\"circle moving down-left\", \"circle\", \"down_left\"),  # Move circle down-left\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDrMR3r7AuhE"
   },
   "source": [
    "Code some mathematical equations to move that circle based on the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCsxxPXrAuhE"
   },
   "outputs": [],
   "source": [
    "def create_image_with_moving_shape(size, frame_num, shape, direction):\n",
    "    # Create a new RGB image with specified size and white background\n",
    "    img = Image.new('RGB', size, color=(255, 255, 255))\n",
    "\n",
    "    # Create a drawing context for the image\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Calculate the center coordinates of the image\n",
    "    center_x, center_y = size[0] // 2, size[1] // 2\n",
    "    position = (center_x, center_y)\n",
    "\n",
    "    # Define a dictionary mapping directions to their respective position adjustments or image transformations\n",
    "    direction_map = {\n",
    "        # Adjust position downwards based on frame number\n",
    "        \"down\": (0, frame_num * 5 % size[1]),\n",
    "        # Adjust position to the left based on frame number\n",
    "        \"left\": (-frame_num * 5 % size[0], 0),\n",
    "        # Adjust position to the right based on frame number\n",
    "        \"right\": (frame_num * 5 % size[0], 0),\n",
    "        # Adjust position diagonally up and to the right\n",
    "        \"diagonal_up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),\n",
    "        # Adjust position diagonally down and to the left\n",
    "        \"diagonal_down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1]),\n",
    "        # Adjust position diagonally up and to the left\n",
    "        \"diagonal_up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),\n",
    "        # Adjust position diagonally down and to the right\n",
    "        \"diagonal_down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),\n",
    "        # Rotate the image clockwise based on frame number\n",
    "        \"rotate_clockwise\": img.rotate(frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),\n",
    "        # Rotate the image counter-clockwise based on frame number\n",
    "        \"rotate_counter_clockwise\": img.rotate(-frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),\n",
    "        # Adjust position for a bouncing effect vertically\n",
    "        \"bounce_vertical\": (0, center_y - abs(frame_num * 5 % size[1] - center_y)),\n",
    "        # Adjust position for a bouncing effect horizontally\n",
    "        \"bounce_horizontal\": (center_x - abs(frame_num * 5 % size[0] - center_x), 0),\n",
    "        # Adjust position for a zigzag effect vertically\n",
    "        \"zigzag_vertical\": (0, center_y - frame_num * 5 % size[1]) if frame_num % 2 == 0 else (0, center_y + frame_num * 5 % size[1]),\n",
    "        # Adjust position for a zigzag effect horizontally\n",
    "        \"zigzag_horizontal\": (center_x - frame_num * 5 % size[0], center_y) if frame_num % 2 == 0 else (center_x + frame_num * 5 % size[0], center_y),\n",
    "        # Adjust position upwards and to the right based on frame number\n",
    "        \"up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),\n",
    "        # Adjust position upwards and to the left based on frame number\n",
    "        \"up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),\n",
    "        # Adjust position downwards and to the right based on frame number\n",
    "        \"down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),\n",
    "        # Adjust position downwards and to the left based on frame number\n",
    "        \"down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1])\n",
    "    }\n",
    "\n",
    "    # Check if direction is in the direction map\n",
    "    if direction in direction_map:\n",
    "        # Check if the direction maps to a position adjustment\n",
    "        if isinstance(direction_map[direction], tuple):\n",
    "            # Update position based on the adjustment\n",
    "            position = tuple(np.add(position, direction_map[direction]))\n",
    "        else:  # If the direction maps to an image transformation\n",
    "            # Update the image based on the transformation\n",
    "            img = direction_map[direction]\n",
    "\n",
    "    # Draw the shape (circle) at the calculated position\n",
    "    if shape == \"circle\":\n",
    "        draw.ellipse([position[0] - shape_size // 2, position[1] - shape_size // 2, position[0] + shape_size // 2, position[1] + shape_size // 2], fill=(0, 0, 255))\n",
    "\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKOxvav7AuhE"
   },
   "source": [
    "The function above is used to move the circle for each frame based on the selected direction. Run a loop on top of it up to the number of videos times to generate all videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6gdhQe4AuhE"
   },
   "outputs": [],
   "source": [
    "for i in range(num_videos):\n",
    "    # Randomly choose a prompt and movement from the predefined list\n",
    "    prompt, shape, direction = random.choice(prompts_and_movements)\n",
    "\n",
    "    # Create a directory for the current video\n",
    "    video_dir = f'training_dataset/video_{i}'\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    # Write the chosen prompt to a text file in the video directory\n",
    "    with open(f'{video_dir}/prompt.txt', 'w') as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    # Generate frames for the current video\n",
    "    for frame_num in range(frames_per_video):\n",
    "        # Create an image with a moving shape based on the current frame number, shape, and direction\n",
    "        img = create_image_with_moving_shape(img_size, frame_num, shape, direction)\n",
    "\n",
    "        # Save the generated image as a PNG file in the video directory\n",
    "        cv2.imwrite(f'{video_dir}/frame_{frame_num}.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCJNxTkGAuhE"
   },
   "source": [
    "The training dataset hasn’t included the **motion of the circle moving up and then to the right**. This will be used as testing prompt for unseen data.\n",
    "\n",
    "The training dataset contains many samples where objects moving away from the scene or appear partially in front of the camera. This help test whether the model can maintain consistency when the circle enters the scene from the very corner without breaking its shape.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RP5M_TEt2H4Mo6OhnlcRLA.gif' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57EpiBaGAuhE"
   },
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUS5INfXAuhF"
   },
   "outputs": [],
   "source": [
    "class TextToVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Initialize the dataset with root directory and optional transform\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # List all subdirectories in the root directory\n",
    "        self.video_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        # Initialize lists to store frame paths and corresponding prompts\n",
    "        self.frame_paths = []\n",
    "        self.prompts = []\n",
    "\n",
    "        for video_dir in self.video_dirs:\n",
    "            # List all PNG files in the video directory and store their paths\n",
    "            frames = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.png')]\n",
    "            self.frame_paths.extend(frames)\n",
    "            # Read the prompt text file in the video directory and store its content\n",
    "            with open(os.path.join(video_dir, 'prompt.txt'), 'r') as f:\n",
    "                prompt = f.read().strip()\n",
    "            # Repeat the prompt for each frame in the video and store in prompts list\n",
    "            self.prompts.extend([prompt] * len(frames))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "    # Retrieve a sample from the dataset given an index\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the prompt corresponding to the given index\n",
    "        frame_path = self.frame_paths[idx]\n",
    "        image = Image.open(frame_path)\n",
    "        prompt = self.prompts[idx]\n",
    "\n",
    "        # Apply transformation if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VKvP7nxAuhF"
   },
   "outputs": [],
   "source": [
    "# Define a set of transformations to be applied to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize image with mean and standard deviation\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = TextToVideoDataset(root_dir='training_dataset', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrpelMLDAuhF"
   },
   "source": [
    "## Text Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "njr9k_thAuhF"
   },
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-oxYMRKAuhF"
   },
   "source": [
    "## Generator Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ4eOTBjAuhG"
   },
   "source": [
    "The `Generator` is responsible for creating video frames from a combination of random noise and text embeddings. It aims to produce realistic video frames conditioned on the given text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4u9jVG_AuhG"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, text_embed_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Fully connected layer that takes noise and text embedding as input\n",
    "        self.fc1 = nn.Linear(100 + text_embed_size, 256 * 8 * 8)\n",
    "\n",
    "        # Transposed convolutional layers to upsample the input\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)  # Output has 3 channels for RGB images\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise, text_embed):\n",
    "        # Concatenate noise and text embedding along the channel dimension\n",
    "        x = torch.cat((noise, text_embed), dim=1)\n",
    "\n",
    "        # Fully connected layer followed by reshaping to 4D tensor\n",
    "        x = self.fc1(x).view(-1, 256, 8, 8)\n",
    "\n",
    "        # Upsampling through transposed convolution layers with ReLU activation\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.relu(self.deconv2(x))\n",
    "\n",
    "        # Final layer with Tanh activation to ensure output values are between -1 and 1 (for images)\n",
    "        x = self.tanh(self.deconv3(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjU_XntgAuhG"
   },
   "source": [
    "## Discriminator Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I5iB-r6AuhG"
   },
   "source": [
    "The `Discriminator` class functions as a binary classifier that distinguishes between real and generated video frames. Its purpose is to evaluate the authenticity of video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfbVv9HKAuhG"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Convolutional layers to process input images\n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)    # 3 input channels (RGB), 64 output channels, kernel size 4x4, stride 2, padding 1\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)  # 64 input channels, 128 output channels, kernel size 4x4, stride 2, padding 1\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 128 input channels, 256 output channels, kernel size 4x4, stride 2, padding 1\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 1)  # Input size 256x8x8 (output size of last convolution), output size 1 (binary classification)\n",
    "\n",
    "        # Activation functions\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)  # negative slope 0.2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Pass input through convolutional layers with LeakyReLU activation\n",
    "        x = self.leaky_relu(self.conv1(input))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "\n",
    "        # Flatten the output of convolutional layers\n",
    "        x = x.view(-1, 256 * 8 * 8)\n",
    "\n",
    "        # Pass through fully connected layer with Sigmoid activation for binary classification\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhi65c5tAuhH"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLx6MPE6AuhH"
   },
   "source": [
    "Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdgbT_gfAuhH"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a simple vocabulary for text prompts\n",
    "all_prompts = [prompt for prompt, _, _ in prompts_and_movements]  # Extract all prompts from prompts_and_movements list\n",
    "vocab = {word: idx for idx, word in enumerate(set(\" \".join(all_prompts).split()))}  # Create a vocabulary dictionary where each unique word is assigned an index\n",
    "vocab_size = len(vocab)  # Size of the vocabulary\n",
    "embed_size = 10  # Size of the text embedding vector\n",
    "\n",
    "def encode_text(prompt):\n",
    "    # Encode a given prompt into a tensor of indices using the vocabulary\n",
    "    return torch.tensor([vocab[word] for word in prompt.split()])\n",
    "\n",
    "# Initialize models, loss function, and optimizers\n",
    "text_embedding = TextEmbedding(vocab_size, embed_size).to(device)\n",
    "netG = Generator(embed_size).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBrYUBbrAuhN"
   },
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTyjHgRbAuhO",
    "outputId": "15511a45-f455-4aca-bd8d-b86446407abb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [05:58<1:11:43, 358.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/13] Loss D: 1.000091552734375, Loss G: 1.2070484161376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 15%|█▌        | 2/13 [12:02<1:06:15, 361.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/13] Loss D: 1.1845459938049316, Loss G: 1.0169572830200195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 23%|██▎       | 3/13 [18:04<1:00:20, 362.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/13] Loss D: 1.1252405643463135, Loss G: 0.9279786348342896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 31%|███       | 4/13 [24:01<54:00, 360.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/13] Loss D: 1.079437255859375, Loss G: 0.9062992334365845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 38%|███▊      | 5/13 [29:45<47:12, 354.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/13] Loss D: 1.0015060901641846, Loss G: 1.0048640966415405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 46%|████▌     | 6/13 [35:25<40:45, 349.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/13] Loss D: 1.0681159496307373, Loss G: 1.0402806997299194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 54%|█████▍    | 7/13 [41:11<34:49, 348.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/13] Loss D: 1.1377320289611816, Loss G: 0.9768543243408203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 62%|██████▏   | 8/13 [46:50<28:47, 345.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/13] Loss D: 1.1480485200881958, Loss G: 0.9558865427970886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 69%|██████▉   | 9/13 [52:33<22:58, 344.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/13] Loss D: 1.3059004545211792, Loss G: 0.930357038974762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 77%|███████▋  | 10/13 [58:12<17:08, 342.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/13] Loss D: 0.951496958732605, Loss G: 1.018438458442688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 85%|████████▍ | 11/13 [1:03:54<11:25, 342.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/13] Loss D: 0.9797308444976807, Loss G: 0.9381852746009827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 92%|█████████▏| 12/13 [1:09:36<05:42, 342.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/13] Loss D: 1.0248430967330933, Loss G: 0.9506300687789917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [1:15:16<00:00, 347.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/13] Loss D: 0.9496458768844604, Loss G: 1.087607502937317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 13\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i, (data, prompts) in enumerate(dataloader):\n",
    "        real_data = data.to(device)\n",
    "\n",
    "        # Convert prompts to list\n",
    "        prompts = [prompt for prompt in prompts]\n",
    "\n",
    "        # Update Discriminator\n",
    "        netD.zero_grad()\n",
    "        batch_size = real_data.size(0)\n",
    "        labels = torch.ones(batch_size, 1).to(device)  # Create labels for real data (ones)\n",
    "        output = netD(real_data)  # Forward pass real data through Discriminator\n",
    "        lossD_real = criterion(output, labels)  # Calculate loss on real data\n",
    "        lossD_real.backward()  # Calculate gradients\n",
    "\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, 100).to(device)  # Generate random noise\n",
    "        text_embeds = torch.stack([text_embedding(encode_text(prompt).to(device)).mean(dim=0) for prompt in prompts])  # Encode prompts into text embeddings\n",
    "        fake_data = netG(noise, text_embeds)  # Generate fake data from noise and text embeddings\n",
    "        labels = torch.zeros(batch_size, 1).to(device)  # Create labels for fake data (zeros)\n",
    "        output = netD(fake_data.detach())  # Forward pass fake data through Discriminator (detach to avoid gradients flowing back to Generator)\n",
    "        lossD_fake = criterion(output, labels)  # Calculate loss on fake data\n",
    "        lossD_fake.backward()  # Calculate gradients\n",
    "        optimizerD.step()  # Update Discriminator parameters\n",
    "\n",
    "        # Update Generator\n",
    "        netG.zero_grad()  # Zero the gradients of the Generator\n",
    "        labels = torch.ones(batch_size, 1).to(device)  # Create labels for fake data (ones) to fool Discriminator\n",
    "        output = netD(fake_data)  # Forward pass fake data (now updated) through Discriminator\n",
    "        lossG = criterion(output, labels)  # Calculate loss for Generator based on Discriminator's response\n",
    "        lossG.backward()  # Calculate gradients\n",
    "        optimizerG.step()  # Update Generator parameters\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDMGHeeHAuhO"
   },
   "source": [
    "Through backpropagation, the loss will be adjusted for both the generator and discriminator.\n",
    "\n",
    "There is a high risk of overfitting. If we had a more diverse dataset, we could consider using higher epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGo4BX-xAuhP"
   },
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dkLXf1nLAuhP"
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), './model/generator.pth')\n",
    "torch.save(netD.state_dict(), './model/discriminator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13OrhfXLAuhP"
   },
   "source": [
    "## Generating AI Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v9BBt6KAuhP"
   },
   "source": [
    "The motion where the circle moves up and then to the right is not present in the training data, so the model is unfamiliar with this specific motion. However, it has been trained on other motions, so use this motion as a prompt to test the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mPVSpYwxAuhP"
   },
   "outputs": [],
   "source": [
    "def generate_video(text_prompt, num_frames=10):\n",
    "    # Create a directory for the generated video frames based on the text prompt\n",
    "    os.makedirs(f'generated_video_{text_prompt.replace(\" \", \"_\")}', exist_ok=True)\n",
    "\n",
    "    # Encode the text prompt into a text embedding tensor\n",
    "    text_embed = text_embedding(encode_text(text_prompt).to(device)).mean(dim=0).unsqueeze(0)\n",
    "\n",
    "    # Generate frames for the video\n",
    "    for frame_num in range(num_frames):\n",
    "        # Generate random noise\n",
    "        noise = torch.randn(1, 100).to(device)\n",
    "\n",
    "        # Generate a fake frame using the Generator network\n",
    "        with torch.no_grad():\n",
    "            fake_frame = netG(noise, text_embed)\n",
    "\n",
    "        # Save the generated fake frame as an image file\n",
    "        save_image(fake_frame, f'generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png')\n",
    "\n",
    "generate_video('circle moving up-right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DslcpFMwAuhQ"
   },
   "source": [
    "Merge all the frames of the generated video into a single short video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_CJMQe-AuhQ"
   },
   "outputs": [],
   "source": [
    "# Define the path to folder containing the PNG frames\n",
    "folder_path = 'generated_video_circle_moving_up-right'\n",
    "\n",
    "# Get the list of all PNG files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "\n",
    "# Sort the images by name (they are numbered sequentially)\n",
    "image_files.sort()\n",
    "\n",
    "# Create a list to store the frames\n",
    "frames = []\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    frame = cv2.imread(image_path)\n",
    "    frames.append(frame)\n",
    "\n",
    "frames = np.array(frames)\n",
    "fps = 10 # frames per second\n",
    "\n",
    "# Create a video writer object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('generated_video.avi', fourcc, fps, (frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "# Write each frame to the video\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the video writer\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzhcyUyGAuhQ"
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH6C__sXAuhQ"
   },
   "source": [
    "[Building an AI Text-to-Video Model from Scratch Using Python](https://levelup.gitconnected.com/building-an-ai-text-to-video-model-from-scratch-using-python-35b4eb4002de)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
